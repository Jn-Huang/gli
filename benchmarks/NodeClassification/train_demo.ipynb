{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo training on cora, node classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gli\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "from gli.utils import to_dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gli.dataloading.get_gli_dataset(\"cora\", \"NodeClassification\")\n",
    "g = data[0]\n",
    "# convert sparse tensor to dense\n",
    "g = to_dense(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load features, labels, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = g.ndata[\"NodeFeature\"]\n",
    "labels = g.ndata[\"NodeLabel\"]\n",
    "train_mask = g.ndata[\"train_mask\"]\n",
    "val_mask = g.ndata[\"val_mask\"]\n",
    "test_mask = g.ndata[\"test_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Data statistics------'\n",
      "    #Edges 10556\n",
      "    #Classes 7\n",
      "    #Train samples 140\n",
      "    #Val samples 500\n",
      "    #Test samples 1000\n"
     ]
    }
   ],
   "source": [
    "in_feats = features.shape[1]\n",
    "n_classes = data.num_labels\n",
    "n_edges = g.number_of_edges()\n",
    "\n",
    "print(f\"\"\"----Data statistics------'\n",
    "    #Edges {n_edges}\n",
    "    #Classes {n_classes}\n",
    "    #Train samples {train_mask.int().sum().item()}\n",
    "    #Val samples {val_mask.int().sum().item()}\n",
    "    #Test samples {test_mask.int().sum().item()}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, labels):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    _, indices = torch.max(logits, dim=1)\n",
    "    correct = torch.sum(indices == labels)\n",
    "    return correct.item() * 1.0 / len(labels)\n",
    "\n",
    "\n",
    "def evaluate(model, features, labels, mask, eval_func):\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        return eval_func(logits, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a GCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"GCN network.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout):\n",
    "        \"\"\"Initiate model.\"\"\"\n",
    "        super().__init__()\n",
    "        self.g = g\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input layer\n",
    "        self.layers.append(GraphConv(in_feats, n_hidden,\n",
    "                                     activation=activation))\n",
    "        # hidden layers\n",
    "        for _ in range(n_layers - 2):\n",
    "            self.layers.append(GraphConv(n_hidden, n_hidden,\n",
    "                                         activation=activation))\n",
    "        # output layer\n",
    "        self.layers.append(GraphConv(n_hidden, n_classes))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"Forward.\"\"\"\n",
    "        h = features\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                h = self.dropout(h)\n",
    "            h = layer(self.g, h)\n",
    "        return h\n",
    "\n",
    "model = GCN(g=g,\n",
    "            in_feats=in_feats,\n",
    "            n_hidden=8,\n",
    "            n_classes=n_classes,\n",
    "            n_layers=2,\n",
    "            activation=F.relu,\n",
    "            dropout=.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare optimizer, evaluation function and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=.01, weight_decay=.001)\n",
    "eval_func = accuracy\n",
    "loss_fcn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model for 200 epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 1.9457 | TrainAcc 0.1000 | ValAcc 0.0760\n",
      "Epoch 00001 | Loss 1.9419 | TrainAcc 0.2071 | ValAcc 0.1040\n",
      "Epoch 00002 | Loss 1.9352 | TrainAcc 0.3286 | ValAcc 0.1940\n",
      "Epoch 00003 | Loss 1.9299 | TrainAcc 0.3571 | ValAcc 0.2820\n",
      "Epoch 00004 | Loss 1.9232 | TrainAcc 0.4071 | ValAcc 0.3440\n",
      "Epoch 00005 | Loss 1.9170 | TrainAcc 0.4214 | ValAcc 0.4180\n",
      "Epoch 00006 | Loss 1.9054 | TrainAcc 0.4571 | ValAcc 0.4580\n",
      "Epoch 00007 | Loss 1.9073 | TrainAcc 0.3571 | ValAcc 0.4940\n",
      "Epoch 00008 | Loss 1.8970 | TrainAcc 0.4214 | ValAcc 0.5420\n",
      "Epoch 00009 | Loss 1.8878 | TrainAcc 0.4643 | ValAcc 0.5260\n",
      "Epoch 00010 | Loss 1.8813 | TrainAcc 0.4714 | ValAcc 0.4900\n",
      "Epoch 00011 | Loss 1.8766 | TrainAcc 0.4357 | ValAcc 0.4940\n",
      "Epoch 00012 | Loss 1.8466 | TrainAcc 0.5143 | ValAcc 0.4900\n",
      "Epoch 00013 | Loss 1.8469 | TrainAcc 0.5071 | ValAcc 0.4960\n",
      "Epoch 00014 | Loss 1.8260 | TrainAcc 0.4857 | ValAcc 0.5120\n",
      "Epoch 00015 | Loss 1.8118 | TrainAcc 0.5857 | ValAcc 0.5400\n",
      "Epoch 00016 | Loss 1.8273 | TrainAcc 0.4929 | ValAcc 0.5480\n",
      "Epoch 00017 | Loss 1.8127 | TrainAcc 0.5214 | ValAcc 0.5560\n",
      "Epoch 00018 | Loss 1.7990 | TrainAcc 0.5286 | ValAcc 0.5560\n",
      "Epoch 00019 | Loss 1.7636 | TrainAcc 0.6000 | ValAcc 0.5520\n",
      "Epoch 00020 | Loss 1.7664 | TrainAcc 0.5286 | ValAcc 0.5520\n",
      "Epoch 00021 | Loss 1.7627 | TrainAcc 0.5071 | ValAcc 0.5580\n",
      "Epoch 00022 | Loss 1.7517 | TrainAcc 0.5286 | ValAcc 0.5560\n",
      "Epoch 00023 | Loss 1.7353 | TrainAcc 0.5000 | ValAcc 0.5660\n",
      "Epoch 00024 | Loss 1.7229 | TrainAcc 0.6000 | ValAcc 0.5720\n",
      "Epoch 00025 | Loss 1.7099 | TrainAcc 0.5571 | ValAcc 0.5760\n",
      "Epoch 00026 | Loss 1.6888 | TrainAcc 0.6357 | ValAcc 0.5900\n",
      "Epoch 00027 | Loss 1.6909 | TrainAcc 0.5357 | ValAcc 0.6020\n",
      "Epoch 00028 | Loss 1.6554 | TrainAcc 0.5857 | ValAcc 0.6020\n",
      "Epoch 00029 | Loss 1.6415 | TrainAcc 0.6071 | ValAcc 0.6180\n",
      "Epoch 00030 | Loss 1.6412 | TrainAcc 0.5857 | ValAcc 0.6160\n",
      "Epoch 00031 | Loss 1.6030 | TrainAcc 0.6071 | ValAcc 0.6240\n",
      "Epoch 00032 | Loss 1.6429 | TrainAcc 0.5857 | ValAcc 0.6220\n",
      "Epoch 00033 | Loss 1.6276 | TrainAcc 0.5786 | ValAcc 0.6240\n",
      "Epoch 00034 | Loss 1.5817 | TrainAcc 0.6286 | ValAcc 0.6260\n",
      "Epoch 00035 | Loss 1.5988 | TrainAcc 0.6071 | ValAcc 0.6260\n",
      "Epoch 00036 | Loss 1.5267 | TrainAcc 0.6643 | ValAcc 0.6200\n",
      "Epoch 00037 | Loss 1.5596 | TrainAcc 0.6214 | ValAcc 0.6280\n",
      "Epoch 00038 | Loss 1.5405 | TrainAcc 0.6214 | ValAcc 0.6440\n",
      "Epoch 00039 | Loss 1.5185 | TrainAcc 0.6214 | ValAcc 0.6460\n",
      "Epoch 00040 | Loss 1.4884 | TrainAcc 0.6857 | ValAcc 0.6500\n",
      "Epoch 00041 | Loss 1.4854 | TrainAcc 0.6143 | ValAcc 0.6520\n",
      "Epoch 00042 | Loss 1.4481 | TrainAcc 0.7000 | ValAcc 0.6600\n",
      "Epoch 00043 | Loss 1.4561 | TrainAcc 0.6500 | ValAcc 0.6580\n",
      "Epoch 00044 | Loss 1.4601 | TrainAcc 0.6643 | ValAcc 0.6560\n",
      "Epoch 00045 | Loss 1.4071 | TrainAcc 0.6643 | ValAcc 0.6540\n",
      "Epoch 00046 | Loss 1.3774 | TrainAcc 0.7214 | ValAcc 0.6560\n",
      "Epoch 00047 | Loss 1.4016 | TrainAcc 0.7143 | ValAcc 0.6540\n",
      "Epoch 00048 | Loss 1.3421 | TrainAcc 0.7357 | ValAcc 0.6560\n",
      "Epoch 00049 | Loss 1.3869 | TrainAcc 0.6929 | ValAcc 0.6680\n",
      "Epoch 00050 | Loss 1.3576 | TrainAcc 0.7071 | ValAcc 0.6680\n",
      "Epoch 00051 | Loss 1.4112 | TrainAcc 0.6643 | ValAcc 0.6680\n",
      "Epoch 00052 | Loss 1.3364 | TrainAcc 0.7143 | ValAcc 0.6760\n",
      "Epoch 00053 | Loss 1.2872 | TrainAcc 0.7071 | ValAcc 0.6760\n",
      "Epoch 00054 | Loss 1.3144 | TrainAcc 0.6500 | ValAcc 0.6800\n",
      "Epoch 00055 | Loss 1.2637 | TrainAcc 0.7143 | ValAcc 0.6820\n",
      "Epoch 00056 | Loss 1.2742 | TrainAcc 0.6500 | ValAcc 0.6880\n",
      "Epoch 00057 | Loss 1.2634 | TrainAcc 0.7500 | ValAcc 0.6840\n",
      "Epoch 00058 | Loss 1.2112 | TrainAcc 0.7214 | ValAcc 0.6800\n",
      "Epoch 00059 | Loss 1.1533 | TrainAcc 0.8000 | ValAcc 0.6800\n",
      "Epoch 00060 | Loss 1.1692 | TrainAcc 0.8000 | ValAcc 0.6800\n",
      "Epoch 00061 | Loss 1.2142 | TrainAcc 0.7429 | ValAcc 0.6880\n",
      "Epoch 00062 | Loss 1.1854 | TrainAcc 0.7143 | ValAcc 0.6860\n",
      "Epoch 00063 | Loss 1.1189 | TrainAcc 0.7643 | ValAcc 0.6840\n",
      "Epoch 00064 | Loss 1.1543 | TrainAcc 0.7429 | ValAcc 0.6820\n",
      "Epoch 00065 | Loss 1.1332 | TrainAcc 0.7429 | ValAcc 0.6920\n",
      "Epoch 00066 | Loss 1.1481 | TrainAcc 0.6929 | ValAcc 0.6940\n",
      "Epoch 00067 | Loss 1.1393 | TrainAcc 0.6857 | ValAcc 0.7000\n",
      "Epoch 00068 | Loss 1.0656 | TrainAcc 0.7929 | ValAcc 0.7000\n",
      "Epoch 00069 | Loss 1.0254 | TrainAcc 0.7714 | ValAcc 0.7000\n",
      "Epoch 00070 | Loss 1.0377 | TrainAcc 0.7929 | ValAcc 0.7020\n",
      "Epoch 00071 | Loss 0.9661 | TrainAcc 0.8071 | ValAcc 0.7000\n",
      "Epoch 00072 | Loss 1.0147 | TrainAcc 0.7357 | ValAcc 0.6980\n",
      "Epoch 00073 | Loss 1.0180 | TrainAcc 0.7643 | ValAcc 0.7040\n",
      "Epoch 00074 | Loss 1.0082 | TrainAcc 0.7571 | ValAcc 0.7040\n",
      "Epoch 00075 | Loss 1.0610 | TrainAcc 0.6929 | ValAcc 0.7080\n",
      "Epoch 00076 | Loss 0.9173 | TrainAcc 0.7714 | ValAcc 0.7080\n",
      "Epoch 00077 | Loss 0.9550 | TrainAcc 0.7500 | ValAcc 0.7080\n",
      "Epoch 00078 | Loss 0.9053 | TrainAcc 0.8000 | ValAcc 0.7080\n",
      "Epoch 00079 | Loss 0.9469 | TrainAcc 0.8071 | ValAcc 0.7060\n",
      "Epoch 00080 | Loss 0.9544 | TrainAcc 0.7714 | ValAcc 0.7100\n",
      "Epoch 00081 | Loss 0.9146 | TrainAcc 0.7786 | ValAcc 0.7140\n",
      "Epoch 00082 | Loss 0.9494 | TrainAcc 0.7571 | ValAcc 0.7140\n",
      "Epoch 00083 | Loss 0.9274 | TrainAcc 0.7500 | ValAcc 0.7140\n",
      "Epoch 00084 | Loss 0.9390 | TrainAcc 0.7500 | ValAcc 0.7180\n",
      "Epoch 00085 | Loss 0.8871 | TrainAcc 0.8000 | ValAcc 0.7180\n",
      "Epoch 00086 | Loss 0.9054 | TrainAcc 0.7571 | ValAcc 0.7160\n",
      "Epoch 00087 | Loss 0.8605 | TrainAcc 0.8214 | ValAcc 0.7060\n",
      "Epoch 00088 | Loss 0.8448 | TrainAcc 0.8000 | ValAcc 0.7060\n",
      "Epoch 00089 | Loss 0.8190 | TrainAcc 0.8071 | ValAcc 0.7020\n",
      "Epoch 00090 | Loss 0.7914 | TrainAcc 0.8071 | ValAcc 0.7040\n",
      "Epoch 00091 | Loss 0.8656 | TrainAcc 0.7571 | ValAcc 0.7080\n",
      "Epoch 00092 | Loss 0.7592 | TrainAcc 0.8143 | ValAcc 0.7120\n",
      "Epoch 00093 | Loss 0.8385 | TrainAcc 0.7786 | ValAcc 0.7180\n",
      "Epoch 00094 | Loss 0.8128 | TrainAcc 0.7643 | ValAcc 0.7260\n",
      "Epoch 00095 | Loss 0.8087 | TrainAcc 0.8214 | ValAcc 0.7260\n",
      "Epoch 00096 | Loss 0.8321 | TrainAcc 0.7357 | ValAcc 0.7280\n",
      "Epoch 00097 | Loss 0.7724 | TrainAcc 0.7714 | ValAcc 0.7260\n",
      "Epoch 00098 | Loss 0.7977 | TrainAcc 0.8143 | ValAcc 0.7240\n",
      "Epoch 00099 | Loss 0.7301 | TrainAcc 0.8000 | ValAcc 0.7240\n",
      "Epoch 00100 | Loss 0.8242 | TrainAcc 0.7714 | ValAcc 0.7200\n",
      "Epoch 00101 | Loss 0.7363 | TrainAcc 0.8143 | ValAcc 0.7200\n",
      "Epoch 00102 | Loss 0.7578 | TrainAcc 0.8286 | ValAcc 0.7220\n",
      "Epoch 00103 | Loss 0.7033 | TrainAcc 0.7857 | ValAcc 0.7320\n",
      "Epoch 00104 | Loss 0.7128 | TrainAcc 0.8071 | ValAcc 0.7340\n",
      "Epoch 00105 | Loss 0.7422 | TrainAcc 0.8214 | ValAcc 0.7380\n",
      "Epoch 00106 | Loss 0.7058 | TrainAcc 0.8143 | ValAcc 0.7420\n",
      "Epoch 00107 | Loss 0.7206 | TrainAcc 0.8214 | ValAcc 0.7420\n",
      "Epoch 00108 | Loss 0.7236 | TrainAcc 0.7857 | ValAcc 0.7440\n",
      "Epoch 00109 | Loss 0.6903 | TrainAcc 0.8286 | ValAcc 0.7480\n",
      "Epoch 00110 | Loss 0.7574 | TrainAcc 0.7500 | ValAcc 0.7480\n",
      "Epoch 00111 | Loss 0.6169 | TrainAcc 0.8643 | ValAcc 0.7480\n",
      "Epoch 00112 | Loss 0.6630 | TrainAcc 0.8286 | ValAcc 0.7520\n",
      "Epoch 00113 | Loss 0.6653 | TrainAcc 0.8214 | ValAcc 0.7500\n",
      "Epoch 00114 | Loss 0.6776 | TrainAcc 0.8214 | ValAcc 0.7560\n",
      "Epoch 00115 | Loss 0.7017 | TrainAcc 0.7714 | ValAcc 0.7560\n",
      "Epoch 00116 | Loss 0.7190 | TrainAcc 0.7929 | ValAcc 0.7580\n",
      "Epoch 00117 | Loss 0.6783 | TrainAcc 0.7929 | ValAcc 0.7580\n",
      "Epoch 00118 | Loss 0.7046 | TrainAcc 0.8214 | ValAcc 0.7520\n",
      "Epoch 00119 | Loss 0.5436 | TrainAcc 0.8500 | ValAcc 0.7520\n",
      "Epoch 00120 | Loss 0.6420 | TrainAcc 0.8286 | ValAcc 0.7520\n",
      "Epoch 00121 | Loss 0.6520 | TrainAcc 0.8214 | ValAcc 0.7520\n",
      "Epoch 00122 | Loss 0.6664 | TrainAcc 0.8143 | ValAcc 0.7500\n",
      "Epoch 00123 | Loss 0.6433 | TrainAcc 0.8214 | ValAcc 0.7500\n",
      "Epoch 00124 | Loss 0.6363 | TrainAcc 0.8429 | ValAcc 0.7480\n",
      "Epoch 00125 | Loss 0.6381 | TrainAcc 0.8214 | ValAcc 0.7480\n",
      "Epoch 00126 | Loss 0.6231 | TrainAcc 0.8357 | ValAcc 0.7500\n",
      "Epoch 00127 | Loss 0.6396 | TrainAcc 0.8000 | ValAcc 0.7440\n",
      "Epoch 00128 | Loss 0.6365 | TrainAcc 0.8571 | ValAcc 0.7420\n",
      "Epoch 00129 | Loss 0.5509 | TrainAcc 0.8643 | ValAcc 0.7440\n",
      "Epoch 00130 | Loss 0.5985 | TrainAcc 0.8357 | ValAcc 0.7420\n",
      "Epoch 00131 | Loss 0.6086 | TrainAcc 0.8000 | ValAcc 0.7400\n",
      "Epoch 00132 | Loss 0.6014 | TrainAcc 0.8500 | ValAcc 0.7480\n",
      "Epoch 00133 | Loss 0.5764 | TrainAcc 0.8500 | ValAcc 0.7460\n",
      "Epoch 00134 | Loss 0.6267 | TrainAcc 0.8286 | ValAcc 0.7480\n",
      "Epoch 00135 | Loss 0.5588 | TrainAcc 0.8500 | ValAcc 0.7500\n",
      "Epoch 00136 | Loss 0.5452 | TrainAcc 0.8571 | ValAcc 0.7500\n",
      "Epoch 00137 | Loss 0.5968 | TrainAcc 0.8500 | ValAcc 0.7480\n",
      "Epoch 00138 | Loss 0.5529 | TrainAcc 0.8500 | ValAcc 0.7480\n",
      "Epoch 00139 | Loss 0.5299 | TrainAcc 0.8429 | ValAcc 0.7460\n",
      "Epoch 00140 | Loss 0.5587 | TrainAcc 0.8571 | ValAcc 0.7460\n",
      "Epoch 00141 | Loss 0.6038 | TrainAcc 0.7929 | ValAcc 0.7480\n",
      "Epoch 00142 | Loss 0.6200 | TrainAcc 0.7929 | ValAcc 0.7460\n",
      "Epoch 00143 | Loss 0.5319 | TrainAcc 0.8429 | ValAcc 0.7460\n",
      "Epoch 00144 | Loss 0.6004 | TrainAcc 0.7643 | ValAcc 0.7460\n",
      "Epoch 00145 | Loss 0.5025 | TrainAcc 0.8571 | ValAcc 0.7420\n",
      "Epoch 00146 | Loss 0.5852 | TrainAcc 0.8286 | ValAcc 0.7460\n",
      "Epoch 00147 | Loss 0.5402 | TrainAcc 0.8643 | ValAcc 0.7460\n",
      "Epoch 00148 | Loss 0.5513 | TrainAcc 0.8643 | ValAcc 0.7480\n",
      "Epoch 00149 | Loss 0.5311 | TrainAcc 0.8429 | ValAcc 0.7480\n",
      "Epoch 00150 | Loss 0.5310 | TrainAcc 0.8286 | ValAcc 0.7460\n",
      "Epoch 00151 | Loss 0.5411 | TrainAcc 0.8500 | ValAcc 0.7440\n",
      "Epoch 00152 | Loss 0.6044 | TrainAcc 0.7857 | ValAcc 0.7460\n",
      "Epoch 00153 | Loss 0.5928 | TrainAcc 0.8143 | ValAcc 0.7480\n",
      "Epoch 00154 | Loss 0.4783 | TrainAcc 0.8643 | ValAcc 0.7520\n",
      "Epoch 00155 | Loss 0.4777 | TrainAcc 0.8857 | ValAcc 0.7560\n",
      "Epoch 00156 | Loss 0.5436 | TrainAcc 0.8286 | ValAcc 0.7580\n",
      "Epoch 00157 | Loss 0.5274 | TrainAcc 0.8786 | ValAcc 0.7580\n",
      "Epoch 00158 | Loss 0.4641 | TrainAcc 0.8857 | ValAcc 0.7580\n",
      "Epoch 00159 | Loss 0.5563 | TrainAcc 0.8286 | ValAcc 0.7580\n",
      "Epoch 00160 | Loss 0.5798 | TrainAcc 0.8000 | ValAcc 0.7600\n",
      "Epoch 00161 | Loss 0.5105 | TrainAcc 0.8500 | ValAcc 0.7580\n",
      "Epoch 00162 | Loss 0.5358 | TrainAcc 0.8000 | ValAcc 0.7560\n",
      "Epoch 00163 | Loss 0.4709 | TrainAcc 0.8786 | ValAcc 0.7500\n",
      "Epoch 00164 | Loss 0.4423 | TrainAcc 0.8643 | ValAcc 0.7520\n",
      "Epoch 00165 | Loss 0.4745 | TrainAcc 0.8500 | ValAcc 0.7520\n",
      "Epoch 00166 | Loss 0.4356 | TrainAcc 0.8857 | ValAcc 0.7540\n",
      "Epoch 00167 | Loss 0.4893 | TrainAcc 0.8571 | ValAcc 0.7520\n",
      "Epoch 00168 | Loss 0.4629 | TrainAcc 0.8786 | ValAcc 0.7500\n",
      "Epoch 00169 | Loss 0.3840 | TrainAcc 0.9071 | ValAcc 0.7520\n",
      "Epoch 00170 | Loss 0.5330 | TrainAcc 0.8643 | ValAcc 0.7500\n",
      "Epoch 00171 | Loss 0.4624 | TrainAcc 0.9000 | ValAcc 0.7520\n",
      "Epoch 00172 | Loss 0.4624 | TrainAcc 0.8643 | ValAcc 0.7520\n",
      "Epoch 00173 | Loss 0.5118 | TrainAcc 0.8429 | ValAcc 0.7540\n",
      "Epoch 00174 | Loss 0.4616 | TrainAcc 0.8857 | ValAcc 0.7560\n",
      "Epoch 00175 | Loss 0.5359 | TrainAcc 0.8357 | ValAcc 0.7560\n",
      "Epoch 00176 | Loss 0.4175 | TrainAcc 0.8643 | ValAcc 0.7580\n",
      "Epoch 00177 | Loss 0.4169 | TrainAcc 0.8929 | ValAcc 0.7580\n",
      "Epoch 00178 | Loss 0.4977 | TrainAcc 0.8571 | ValAcc 0.7540\n",
      "Epoch 00179 | Loss 0.4572 | TrainAcc 0.8929 | ValAcc 0.7540\n",
      "Epoch 00180 | Loss 0.5015 | TrainAcc 0.8429 | ValAcc 0.7520\n",
      "Epoch 00181 | Loss 0.4707 | TrainAcc 0.8286 | ValAcc 0.7580\n",
      "Epoch 00182 | Loss 0.4777 | TrainAcc 0.8643 | ValAcc 0.7580\n",
      "Epoch 00183 | Loss 0.3935 | TrainAcc 0.9000 | ValAcc 0.7580\n",
      "Epoch 00184 | Loss 0.4955 | TrainAcc 0.8429 | ValAcc 0.7560\n",
      "Epoch 00185 | Loss 0.4256 | TrainAcc 0.8786 | ValAcc 0.7540\n",
      "Epoch 00186 | Loss 0.4767 | TrainAcc 0.8429 | ValAcc 0.7500\n",
      "Epoch 00187 | Loss 0.4365 | TrainAcc 0.8571 | ValAcc 0.7460\n",
      "Epoch 00188 | Loss 0.4061 | TrainAcc 0.8786 | ValAcc 0.7500\n",
      "Epoch 00189 | Loss 0.4320 | TrainAcc 0.8786 | ValAcc 0.7480\n",
      "Epoch 00190 | Loss 0.4186 | TrainAcc 0.8500 | ValAcc 0.7520\n",
      "Epoch 00191 | Loss 0.4296 | TrainAcc 0.8786 | ValAcc 0.7520\n",
      "Epoch 00192 | Loss 0.4230 | TrainAcc 0.8786 | ValAcc 0.7520\n",
      "Epoch 00193 | Loss 0.4111 | TrainAcc 0.9143 | ValAcc 0.7520\n",
      "Epoch 00194 | Loss 0.4203 | TrainAcc 0.8500 | ValAcc 0.7480\n",
      "Epoch 00195 | Loss 0.4252 | TrainAcc 0.8929 | ValAcc 0.7480\n",
      "Epoch 00196 | Loss 0.3882 | TrainAcc 0.8786 | ValAcc 0.7520\n",
      "Epoch 00197 | Loss 0.4695 | TrainAcc 0.8643 | ValAcc 0.7560\n",
      "Epoch 00198 | Loss 0.4263 | TrainAcc 0.8714 | ValAcc 0.7560\n",
      "Epoch 00199 | Loss 0.3740 | TrainAcc 0.9071 | ValAcc 0.7560\n",
      "Test Accuracy: 0.7570\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "        model.train()\n",
    "\n",
    "        # forward\n",
    "        logits = model(features)\n",
    "        loss = loss_fcn(logits[train_mask], labels[train_mask])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_acc = eval_func(logits[train_mask], labels[train_mask])\n",
    "        val_acc = evaluate(model, features, labels, val_mask, eval_func)\n",
    "        print(f\"Epoch {epoch:05d} | Loss {loss.item():.4f} | \"\n",
    "              f\"TrainAcc {train_acc:.4f} | ValAcc {val_acc:.4f}\")\n",
    "\n",
    "test_acc = evaluate(model, features, labels, test_mask, eval_func)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('gli')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "496735e2510eb01598c2e41c87e7158e483456e27064fdd2aa156655e2ef77e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
